{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RVROGq2FJwYi"
      },
      "outputs": [],
      "source": [
        "!pip3 install -q torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nth14yddKzEA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhFi-ZMHLSg_"
      },
      "outputs": [],
      "source": [
        "mnist_dataset = datasets.MNIST(root='../data', train=True, transform = transforms.ToTensor(), download=True)\n",
        "\n",
        "def show(img,renorm=False,nrow=8,interpolation='bicubic'):\n",
        "  if renorm:\n",
        "    img = img/2 + 0.5\n",
        "  img_grid = torchvision.utils.make_grid(img,nrow=nrow).numpy()\n",
        "  plt.figure()\n",
        "  plt.imshow(np.transpose(img_grid, (1,2,0)), interpolation=interpolation)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "image, label =  mnist_dataset.__getitem__(2)\n",
        "show(image)\n",
        "print(label)\n",
        "print(image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgAiVOKmQ6NZ"
      },
      "outputs": [],
      "source": [
        "!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3te-J7PRIXX"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('celeba.zip','r') as zip_ref:\n",
        "    zip_ref.extractall('data_faces/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcMMATobSoCH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "root = 'data_faces/img_align_celeba'\n",
        "img_list = os.listdir(root)\n",
        "print(img_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBR1R-YDUJjq"
      },
      "outputs": [],
      "source": [
        " def plot_image(align,nrow=2):   \n",
        "    figsize = (20,10)\n",
        "    ncol = 5\n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    N = nrow*ncol\n",
        "    for i, myid in enumerate(align[\"image_id\"][:N]):\n",
        "        image = load_img(dir_data + \"/\" + myid)\n",
        "        image = img_to_array(image)/255.0\n",
        "\n",
        "        (_, \n",
        "         lefteye_x,    lefteye_y,\n",
        "         righteye_x,   righteye_y, \n",
        "         nose_x,       nose_y,\n",
        "         leftmouth_x,  leftmouth_y, \n",
        "         rightmouth_x, rightmouth_y) = align.iloc[i]\n",
        "\n",
        "\n",
        "        ax  = fig.add_subplot(nrow,ncol,i+1)\n",
        "        ax.imshow(image)\n",
        "        ax.set_title(image.shape)\n",
        "        ax.scatter(lefteye_x,    lefteye_y)\n",
        "        ax.scatter(righteye_x,   righteye_y)\n",
        "        ax.scatter(nose_x,       nose_y)\n",
        "        ax.scatter(leftmouth_x,  leftmouth_y)\n",
        "        ax.scatter(rightmouth_x, rightmouth_y)\n",
        "plot_image()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39KoIw8uUn4_"
      },
      "outputs": [],
      "source": [
        "############## 시계열 주가 예측 ############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzSQqu0iUnrf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.offline as offline\n",
        "import plotly.graph_objs as go"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OVRnkO37VOIR"
      },
      "outputs": [],
      "source": [
        "class Machine:\n",
        "    def __init__(self):\n",
        "        self.code_df = pd.DataFrame({'name' : [], 'code' : []})\n",
        "\n",
        "    def krx_crawl(self):\n",
        "        self.code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13',\n",
        "                                    header=0)[0]\n",
        "        self.code_df.종목코드 = self.code_df.종목코드.map('{:06d}'.format)\n",
        "        self.code_df = self.code_df[['회사명','종목코드']]\n",
        "        self.code_df = self.code_df.rename(columns={'회사명':'name','종목코드':'code'})\n",
        "\n",
        "    def createDataframe(self, code):\n",
        "        df = pd.DataFrame()\n",
        "        for page in range(1, 21):\n",
        "            pg_url = 'https://finance.naver.com/item/sise_day.nhn?code={code}&page={page}'.format(code=code, page=page)\n",
        "            df = df.append(pd.read_html(pg_url, header=0)[0], ignore_index=True)\n",
        "            df.dropna(inplace=True)\n",
        "        return df\n",
        "\n",
        "    def rename_item_name(self, param):\n",
        "        df = param.rename(columns = {'날짜':'date', '종가':'close','전일비':'diff',\n",
        "                                           '시가':'open','고가':'high','저가':'low','거래량':'volumn'})\n",
        "        df[['close', 'diff', 'open', 'high', 'low', 'volumn']] = \\\n",
        "            df[['close','diff','open','high','low','volumn']].astype(int)\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        return df.sort_values(by=['date'], ascending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdBBf2KjYMxi"
      },
      "outputs": [],
      "source": [
        "m = Machine()\n",
        "def print_menu():\n",
        "    print('MENU \\n 0. EXIT 1. 종목컬럼 2. 전처리결과')\n",
        "    return input(\"메뉴 선택\")\n",
        "\n",
        "\n",
        "while 1:\n",
        "    menu = print_menu()\n",
        "    print('MENU %s \\n' % menu)\n",
        "    if menu == '0':\n",
        "        break\n",
        "    elif menu == '1':\n",
        "        print(m.createDataframe('005930'))\n",
        "    elif menu == '2':\n",
        "        print(m.rename_item_name(m.createDataframe('005930')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuCdL298h7qa"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "root_dir = \"/content/gdrive/My Drive/\"\n",
        "import pandas as pd\n",
        "df = pd.read_csv(root_dir+'NSE-Tata-Global-Beverages-Limited.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVqi6nQZjBU6"
      },
      "outputs": [],
      "source": [
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH9IeUr6jr3a"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize']=20,10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dropout,Dense\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAffb-Mmjt4H"
      },
      "outputs": [],
      "source": [
        "df['Date'] = pd.to_datetime(df.Date, format=\"%Y-%m-%d\")\n",
        "\n",
        "df.index = df['Date']\n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(df[\"Close\"], label='Close Price history')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBwd64XenPyY"
      },
      "outputs": [],
      "source": [
        "lstm_model = Sequential()\n",
        "lstm_model.add(LSTM(units=50))\n",
        "lstm_model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "lstm_model.save(\"lstm_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sfF4u29OvnL"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Copy of Which Celebrity are You.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1cZT1xR47C_72L5P-uKpSAByFxSp6O6j0\n",
        "\n",
        "# Introduction\n",
        "Please check this article for more background and explanation about the implementation: (https://towardsdatascience.com/which-celebrity-are-you-d8c6507f21c9).\n",
        "\n",
        "# Step 0: Install & load the necessary packages\n",
        "\"\"\"\n",
        "\n",
        "!pip install mtcnn\n",
        "!pip install keras_vggface\n",
        "!pip install tensorflow\n",
        "!pip install keras\n",
        "!pip install opencv\n",
        "!pip install PIL\n",
        "\n",
        "import mtcnn\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from keras_vggface.vggface import VGGFace\n",
        "from keras_vggface.utils import preprocess_input\n",
        "from keras_vggface.utils import decode_predictions\n",
        "import PIL\n",
        "import os\n",
        "from urllib import request\n",
        "import numpy as np\n",
        "import cv2\n",
        "# Import this one if you are working in the google colab environment\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "\"\"\"# Step 1 : Image loading\"\"\"\n",
        "\n",
        "# Give the image link\n",
        "url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Channing_Tatum_by_Gage_Skidmore_3.jpg/330px-Channing_Tatum_by_Gage_Skidmore_3.jpg\"\n",
        "\n",
        "# Open the link and save the image to res\n",
        "res = request.urlopen(url)\n",
        "# Read the res object and convert it to an array\n",
        "img = np.asarray(bytearray(res.read()), dtype='uint8')\n",
        "# Add the color variable\n",
        "img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "# Show the image\n",
        "cv2_imshow(img)\n",
        "\n",
        "\"\"\"# Step 2: Face detection\"\"\"\n",
        "\n",
        "# Initialize mtcnn detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# set face extraction parameters\n",
        "target_size = (224,224) # output image size\n",
        "border_rel = 0 # increase or decrease zoom on image\n",
        "\n",
        "# detect faces in the image\n",
        "detections = detector.detect_faces(img)\n",
        "print(detections)\n",
        "\n",
        "x1, y1, width, height = detections[0]['box']\n",
        "dw = round(width * border_rel)\n",
        "dh = round(height * border_rel)\n",
        "x2, y2 = x1 + width + dw, y1 + height + dh\n",
        "face = img[y1:y2, x1:x2]\n",
        "\n",
        "# resize pixels to the model size\n",
        "face = PIL.Image.fromarray(face)\n",
        "face = face.resize((224, 224))\n",
        "face = np.asarray(face)\n",
        "# show face\n",
        "cv2_imshow(face)\n",
        "\n",
        "\"\"\"# Step 3: Preprocessing\"\"\"\n",
        "\n",
        "# convert to float32\n",
        "face_pp = face.astype('float32')\n",
        "face_pp = np.expand_dims(face_pp, axis = 0)\n",
        "\n",
        "face_pp = preprocess_input(face_pp, version = 2)\n",
        "\n",
        "\"\"\"# Step 4: Predict\"\"\"\n",
        "\n",
        "# Create the resnet50 Model\n",
        "model = VGGFace(model= 'resnet50')\n",
        "# Check what the required input of the model is & output\n",
        "print('Inputs: {input}'.format(input = model.inputs))\n",
        "print('Output: {output}'.format(output = model.outputs))\n",
        "\n",
        "# predict the face with the input\n",
        "prediction = model.predict(face_pp)\n",
        "\n",
        "\"\"\"# Step 5: Extract results\"\"\"\n",
        "\n",
        "# convert predictions into names & probabilities\n",
        "results = decode_predictions(prediction)\n",
        "# Display results\n",
        "cv2_imshow(img)\n",
        "for result in results[0]:\n",
        "  print ('%s: %.3f%%' % (result[0], result[1]*100))\n",
        "\n",
        "\"\"\"# Lets put everything together\"\"\"\n",
        "\n",
        "def get_img(url):\n",
        "  # Open the link and save the image to res\n",
        "  res = request.urlopen(url)\n",
        "  # Read the res object and convert it to an array\n",
        "  img = np.asarray(bytearray(res.read()), dtype='uint8')\n",
        "  # Add the color variable\n",
        "  img = cv2.imdecode(img, cv2.IMREAD_COLOR)\n",
        "  return img\n",
        "\n",
        "def find_face(img, detector = None):\n",
        "  # Initialize mtcnn detector\n",
        "  detector = MTCNN() if detector is None else detector \n",
        "  # set face extraction parameters\n",
        "  target_size = (224,224) # output image size\n",
        "  border_rel = 0 # increase or decrease zoom on image\n",
        "  # detect faces in the image\n",
        "  detections = detector.detect_faces(img)\n",
        "  x1, y1, width, height = detections[0]['box']\n",
        "  dw = round(width * border_rel)\n",
        "  dh = round(height * border_rel)\n",
        "  x2, y2 = x1 + width + dw, y1 + height + dh\n",
        "  face = img[y1:y2, x1:x2]\n",
        "  # resize pixels to the model size\n",
        "  face = PIL.Image.fromarray(face)\n",
        "  face = face.resize((224, 224))\n",
        "  face = np.asarray(face)\n",
        "  return face\n",
        "\n",
        "def preprocess_face(face):\n",
        "  # convert to float32\n",
        "  face_pp = face.astype('float32')\n",
        "  face_pp = np.expand_dims(face_pp, axis = 0)\n",
        "  face_pp = preprocess_input(face_pp, version = 2)\n",
        "  return face_pp\n",
        "\n",
        "def predict(face_pp, model=None):\n",
        "  model = VGGFace(model= 'resnet50') if model is None else model\n",
        "  return model.predict(face_pp)\n",
        "\n",
        "def extract_and_display_results(prediction, img):\n",
        "  # convert predictions into names & probabilities\n",
        "  results = decode_predictions(prediction)\n",
        "  # Display results\n",
        "  cv2_imshow(img)\n",
        "  for result in results[0]:\n",
        "    print ('%s: %.3f%%' % (result[0], result[1]*100))\n",
        "  return results\n",
        "\n",
        "def predict_from_image_url(url, face_detector=None, model=None):\n",
        "  img = get_img(url)\n",
        "  face = find_face(img, detector=face_detector)\n",
        "  face_pp = preprocess_face(face)\n",
        "  prediction = predict(face_pp, model=model)\n",
        "  return extract_and_display_results(prediction, img)\n",
        "\n",
        "URL = \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8d/Channing_Tatum_by_Gage_Skidmore_3.jpg/330px-Channing_Tatum_by_Gage_Skidmore_3.jpg\"\n",
        "results = predict_from_image_url(url=URL, face_detector=detector, model=model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFy_5EnKQkUX"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        " \n",
        "# 랜덤에 의해 똑같은 결과를 재현하도록 시드 설정\n",
        "# 하이퍼파라미터를 튜닝하기 위한 용도(흔들리면 무엇때문에 좋아졌는지 알기 어려움)\n",
        "tf.set_random_seed(777)\n",
        " \n",
        " \n",
        "# Standardization\n",
        "def data_standardization(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.mean()) / x_np.std()\n",
        " \n",
        "# 너무 작거나 너무 큰 값이 학습을 방해하는 것을 방지하고자 정규화한다\n",
        "# x가 양수라는 가정하에 최소값과 최대값을 이용하여 0~1사이의 값으로 변환\n",
        "# Min-Max scaling\n",
        "def min_max_scaling(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-7) # 1e-7은 0으로 나누는 오류 예방차원\n",
        " \n",
        "# 정규화된 값을 원래의 값으로 되돌린다\n",
        "# 정규화하기 이전의 org_x값과 되돌리고 싶은 x를 입력하면 역정규화된 값을 리턴한다\n",
        "def reverse_min_max_scaling(org_x, x):\n",
        "    org_x_np = np.asarray(org_x)\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()\n",
        " \n",
        " \n",
        "# 하이퍼파라미터\n",
        "input_data_column_cnt = 6  # 입력데이터의 컬럼 개수(Variable 개수)\n",
        "output_data_column_cnt = 1 # 결과데이터의 컬럼 개수\n",
        " \n",
        "seq_length = 28            # 1개 시퀀스의 길이(시계열데이터 입력 개수)\n",
        "rnn_cell_hidden_dim = 20   # 각 셀의 (hidden)출력 크기\n",
        "forget_bias = 1.0          # 망각편향(기본값 1.0)\n",
        "num_stacked_layers = 1     # stacked LSTM layers 개수\n",
        "keep_prob = 1.0            # dropout할 때 keep할 비율\n",
        " \n",
        "epoch_num = 1000           # 에폭 횟수(학습용전체데이터를 몇 회 반복해서 학습할 것인가 입력)\n",
        "learning_rate = 0.01       # 학습률\n",
        " \n",
        " \n",
        "# 데이터를 로딩한다.\n",
        "stock_file_name = 'AMZN.csv' # 아마존 주가데이터 파일\n",
        "encoding = 'euc-kr' # 문자 인코딩\n",
        "names = ['Date','Open','High','Low','Close','Adj Close','Volume']\n",
        "raw_dataframe = pd.read_csv(stock_file_name, names=names, encoding=encoding) #판다스이용 csv파일 로딩\n",
        "raw_dataframe.info() # 데이터 정보 출력\n",
        " \n",
        "# raw_dataframe.drop('Date', axis=1, inplace=True) # 시간열을 제거하고 dataframe 재생성하지 않기\n",
        "del raw_dataframe['Date'] # 위 줄과 같은 효과\n",
        " \n",
        "stock_info = raw_dataframe.values[1:].astype(np.float) # 금액&거래량 문자열을 부동소수점형으로 변환한다\n",
        "print(\"stock_info.shape: \", stock_info.shape)\n",
        "print(\"stock_info[0]: \", stock_info[0])\n",
        " \n",
        " \n",
        "# 데이터 전처리\n",
        "# 가격과 거래량 수치의 차이가 많아나서 각각 별도로 정규화한다\n",
        " \n",
        "# 가격형태 데이터들을 정규화한다\n",
        "# ['Open','High','Low','Close','Adj Close','Volume']에서 'Adj Close'까지 취함\n",
        "# 곧, 마지막 열 Volume를 제외한 모든 열\n",
        "price = stock_info[:,:-1]\n",
        "norm_price = min_max_scaling(price) # 가격형태 데이터 정규화 처리\n",
        "print(\"price.shape: \", price.shape)\n",
        "print(\"price[0]: \", price[0])\n",
        "print(\"norm_price[0]: \", norm_price[0])\n",
        "print(\"=\"*100) # 화면상 구분용\n",
        " \n",
        "# 거래량형태 데이터를 정규화한다\n",
        "# ['Open','High','Low','Close','Adj Close','Volume']에서 마지막 'Volume'만 취함\n",
        "# [:,-1]이 아닌 [:,-1:]이므로 주의하자! 스칼라가아닌 벡터값 산출해야만 쉽게 병합 가능\n",
        "volume = stock_info[:,-1:]\n",
        "norm_volume = min_max_scaling(volume) # 거래량형태 데이터 정규화 처리\n",
        "print(\"volume.shape: \", volume.shape)\n",
        "print(\"volume[0]: \", volume[0])\n",
        "print(\"norm_volume[0]: \", norm_volume[0])\n",
        "print(\"=\"*100) # 화면상 구분용\n",
        " \n",
        "# 행은 그대로 두고 열을 우측에 붙여 합친다\n",
        "x = np.concatenate((norm_price, norm_volume), axis=1) # axis=1, 세로로 합친다\n",
        "print(\"x.shape: \", x.shape)\n",
        "print(\"x[0]: \", x[0])    # x의 첫 값\n",
        "print(\"x[-1]: \", x[-1])  # x의 마지막 값\n",
        "print(\"=\"*100) # 화면상 구분용\n",
        " \n",
        "y = x[:, [-2]] # 타켓은 주식 종가이다\n",
        "print(\"y[0]: \",y[0])     # y의 첫 값\n",
        "print(\"y[-1]: \",y[-1])   # y의 마지막 값\n",
        " \n",
        " \n",
        "dataX = [] # 입력으로 사용될 Sequence Data\n",
        "dataY = [] # 출력(타켓)으로 사용\n",
        " \n",
        "for i in range(0, len(y) - seq_length):\n",
        "    _x = x[i : i+seq_length]\n",
        "    _y = y[i + seq_length] # 다음 나타날 주가(정답)\n",
        "    if i is 0:\n",
        "        print(_x, \"->\", _y) # 첫번째 행만 출력해 봄\n",
        "    dataX.append(_x) # dataX 리스트에 추가\n",
        "    dataY.append(_y) # dataY 리스트에 추가\n",
        " \n",
        " \n",
        "# 학습용/테스트용 데이터 생성\n",
        "# 전체 70%를 학습용 데이터로 사용\n",
        "train_size = int(len(dataY) * 0.7)\n",
        "# 나머지(30%)를 테스트용 데이터로 사용\n",
        "test_size = len(dataY) - train_size\n",
        " \n",
        "# 데이터를 잘라 학습용 데이터 생성\n",
        "trainX = np.array(dataX[0:train_size])\n",
        "trainY = np.array(dataY[0:train_size])\n",
        " \n",
        "# 데이터를 잘라 테스트용 데이터 생성\n",
        "testX = np.array(dataX[train_size:len(dataX)])\n",
        "testY = np.array(dataY[train_size:len(dataY)])\n",
        " \n",
        " \n",
        "# 텐서플로우 플레이스홀더 생성\n",
        "# 입력 X, 출력 Y를 생성한다\n",
        "X = tf.placeholder(tf.float32, [None, seq_length, input_data_column_cnt])\n",
        "print(\"X: \", X)\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "print(\"Y: \", Y)\n",
        " \n",
        "# 검증용 측정지표를 산출하기 위한 targets, predictions를 생성한다\n",
        "targets = tf.placeholder(tf.float32, [None, 1])\n",
        "print(\"targets: \", targets)\n",
        " \n",
        "predictions = tf.placeholder(tf.float32, [None, 1])\n",
        "print(\"predictions: \", predictions)\n",
        " \n",
        " \n",
        "# 모델(LSTM 네트워크) 생성\n",
        "def lstm_cell():\n",
        "    # LSTM셀을 생성\n",
        "    # num_units: 각 Cell 출력 크기\n",
        "    # forget_bias:  to the biases of the forget gate \n",
        "    #              (default: 1)  in order to reduce the scale of forgetting in the beginning of the training.\n",
        "    # state_is_tuple: True ==> accepted and returned states are 2-tuples of the c_state and m_state.\n",
        "    # state_is_tuple: False ==> they are concatenated along the column axis.\n",
        "    cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_cell_hidden_dim, \n",
        "                                        forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)\n",
        "    if keep_prob < 1.0:\n",
        "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "    return cell\n",
        " \n",
        "# num_stacked_layers개의 층으로 쌓인 Stacked RNNs 생성\n",
        "stackedRNNs = [lstm_cell() for _ in range(num_stacked_layers)]\n",
        "multi_cells = tf.contrib.rnn.MultiRNNCell(stackedRNNs, state_is_tuple=True) if num_stacked_layers > 1 else lstm_cell()\n",
        " \n",
        "# RNN Cell(여기서는 LSTM셀임)들을 연결\n",
        "hypothesis, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
        "print(\"hypothesis: \", hypothesis)\n",
        " \n",
        "# [:, -1]를 잘 살펴보자. LSTM RNN의 마지막 (hidden)출력만을 사용했다.\n",
        "# 과거 여러 거래일의 주가를 이용해서 다음날의 주가 1개를 예측하기때문에 MANY-TO-ONE형태이다\n",
        "hypothesis = tf.contrib.layers.fully_connected(hypothesis[:, -1], output_data_column_cnt, activation_fn=tf.identity)\n",
        " \n",
        " \n",
        "# 손실함수로 평균제곱오차를 사용한다\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - Y))\n",
        "# 최적화함수로 AdamOptimizer를 사용한다\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "# optimizer = tf.train.RMSPropOptimizer(learning_rate) # LSTM과 궁합 별로임\n",
        " \n",
        "train = optimizer.minimize(loss)\n",
        " \n",
        "# RMSE(Root Mean Square Error)\n",
        "# 제곱오차의 평균을 구하고 다시 제곱근을 구하면 평균 오차가 나온다\n",
        "# rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions))) # 아래 코드와 같다\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.squared_difference(targets, predictions)))\n",
        " \n",
        " \n",
        "train_error_summary = [] # 학습용 데이터의 오류를 중간 중간 기록한다\n",
        "test_error_summary = []  # 테스트용 데이터의 오류를 중간 중간 기록한다\n",
        "test_predict = ''        # 테스트용데이터로 예측한 결과\n",
        " \n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        " \n",
        "# 학습한다\n",
        "start_time = datetime.datetime.now() # 시작시간을 기록한다\n",
        "print('학습을 시작합니다...')\n",
        "for epoch in range(epoch_num):\n",
        "    _, _loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
        "    if ((epoch+1) % 100 == 0) or (epoch == epoch_num-1): # 100번째마다 또는 마지막 epoch인 경우\n",
        "        # 학습용데이터로 rmse오차를 구한다\n",
        "        train_predict = sess.run(hypothesis, feed_dict={X: trainX})\n",
        "        train_error = sess.run(rmse, feed_dict={targets: trainY, predictions: train_predict})\n",
        "        train_error_summary.append(train_error)\n",
        " \n",
        "        # 테스트용데이터로 rmse오차를 구한다\n",
        "        test_predict = sess.run(hypothesis, feed_dict={X: testX})\n",
        "        test_error = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
        "        test_error_summary.append(test_error)\n",
        "        \n",
        "        # 현재 오류를 출력한다\n",
        "        print(\"epoch: {}, train_error(A): {}, test_error(B): {}, B-A: {}\".format(epoch+1, train_error, test_error, test_error-train_error))\n",
        "        \n",
        "end_time = datetime.datetime.now() # 종료시간을 기록한다\n",
        "elapsed_time = end_time - start_time # 경과시간을 구한다\n",
        "print('elapsed_time:',elapsed_time)\n",
        "print('elapsed_time per epoch:',elapsed_time/epoch_num)\n",
        " \n",
        " \n",
        "# 하이퍼파라미터 출력\n",
        "print('input_data_column_cnt:', input_data_column_cnt, end='')\n",
        "print(',output_data_column_cnt:', output_data_column_cnt, end='')\n",
        " \n",
        "print(',seq_length:', seq_length, end='')\n",
        "print(',rnn_cell_hidden_dim:', rnn_cell_hidden_dim, end='')\n",
        "print(',forget_bias:', forget_bias, end='')\n",
        "print(',num_stacked_layers:', num_stacked_layers, end='')\n",
        "print(',keep_prob:', keep_prob, end='')\n",
        " \n",
        "print(',epoch_num:', epoch_num, end='')\n",
        "print(',learning_rate:', learning_rate, end='')\n",
        " \n",
        "print(',train_error:', train_error_summary[-1], end='')\n",
        "print(',test_error:', test_error_summary[-1], end='')\n",
        "print(',min_test_error:', np.min(test_error_summary))\n",
        " \n",
        "# 결과 그래프 출력\n",
        "plt.figure(1)\n",
        "plt.plot(train_error_summary, 'gold')\n",
        "plt.plot(test_error_summary, 'b')\n",
        "plt.xlabel('Epoch(x100)')\n",
        "plt.ylabel('Root Mean Square Error')\n",
        " \n",
        "plt.figure(2)\n",
        "plt.plot(testY, 'r')\n",
        "plt.plot(test_predict, 'b')\n",
        "plt.xlabel('Time Period')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.show()\n",
        " \n",
        " \n",
        "# sequence length만큼의 가장 최근 데이터를 슬라이싱한다\n",
        "recent_data = np.array([x[len(x)-seq_length : ]])\n",
        "print(\"recent_data.shape:\", recent_data.shape)\n",
        "print(\"recent_data:\", recent_data)\n",
        " \n",
        "# 내일 종가를 예측해본다\n",
        "test_predict = sess.run(hypothesis, feed_dict={X: recent_data})\n",
        " \n",
        "print(\"test_predict\", test_predict[0])\n",
        "test_predict = reverse_min_max_scaling(price,test_predict) # 금액데이터 역정규화한다\n",
        "print(\"Tomorrow's stock price\", test_predict[0]) # 예측한 주가를 출력한다"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "celeba.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('yoloenv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "11f4621a6099d58d907dd1f6e1cd9ae5f54e214efa265e8bfd71f590918087d0"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
